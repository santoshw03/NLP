{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9892063-0cba-4ef6-abe3-bdc819c4e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization is a process of splittinng the text into meaningful segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b06dacb-1d48-4d86-a7f6-3aeb618ddcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b1d6ca-7da7-42a8-afcc-d818fa039d23",
   "metadata": {},
   "source": [
    "#seperating tokens from doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa06ca5c-cd2c-42be-bb3b-364b923007cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization\n",
      "is\n",
      "the\n",
      "process\n",
      "of\n",
      "splitting\n",
      "a\n",
      "text\n",
      "or\n",
      "a\n",
      "sentence\n",
      "into\n",
      "segments\n",
      ",\n",
      "which\n",
      "are\n",
      "called\n",
      "tokens\n",
      ".\n",
      "It\n",
      "is\n",
      "the\n",
      "first\n",
      "step\n",
      "of\n",
      "text\n",
      "preprocessing\n",
      "and\n",
      "is\n",
      "used\n",
      "as\n",
      "input\n",
      "for\n",
      "subsequent\n",
      "processes\n",
      "like\n",
      "text\n",
      "classification\n",
      ",\n",
      "lemmatization\n",
      ",\n",
      "etc\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"Tokenization is the process of splitting a text or a sentence into segments, which are called tokens. It is the first step of text preprocessing and is used as input for subsequent processes like text classification, lemmatization, etc.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae04c85b-3c5e-4e05-87cf-f45321668049",
   "metadata": {},
   "source": [
    "#Spanning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b4f2052-080d-406d-9dc9-f7d874f1445d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenization"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42a1d758-c7a0-45b5-830d-af93897bc7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is the process of splitting"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09b574ff-0f92-4308-bbc9-fb86b4601238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span = doc[1:6]\n",
    "type(span)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3a44f4-bd57-4f95-a56f-546735165aa8",
   "metadata": {},
   "source": [
    "# tokenization in a data / documnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66e21c07-8e7c-4b6e-9000-1a7581b861c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dayton high school, 8th grade students information\\n',\n",
       " '===========================================================\\n',\n",
       " '\\n',\n",
       " ' Name            birth day                  email\\n',\n",
       " '---------      ----------------      -----------------------\\n',\n",
       " 'Virat            5 June, 1882          virat@kohli.com\\n',\n",
       " '\\n',\n",
       " 'Maria           12 April, 2001         maria@sharapova.com\\n',\n",
       " '\\n',\n",
       " 'Serena          24 June, 1998          serena@williams.com\\n',\n",
       " '\\n',\n",
       " 'Joe              1 May, 1997           joe@root.com']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"students.txt\") as s:\n",
    "    text = s.readlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cce8d4e3-6b55-41f9-8cf9-45ba79989b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dayton high school, 8th grade students information\\n ===========================================================\\n \\n  Name            birth day                  email\\n ---------      ----------------      -----------------------\\n Virat            5 June, 1882          virat@kohli.com\\n \\n Maria           12 April, 2001         maria@sharapova.com\\n \\n Serena          24 June, 1998          serena@williams.com\\n \\n Joe              1 May, 1997           joe@root.com'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#above text is an array of multiple lines , lets first convert it into a single text:\n",
    "text = ' '.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49d76e95-beaf-4336-993d-9b258627e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11f992cc-5390-412d-95e4-c9d30d8ef19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating/Tokenizing all emails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d02c6354-b07a-4c93-96a8-94e4cf336857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['virat@kohli.com',\n",
       " 'maria@sharapova.com',\n",
       " 'serena@williams.com',\n",
       " 'joe@root.com']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails=[]\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text)\n",
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d14a7f6-b674-4a7d-845f-6070a64b0564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2d20a72-ac22-40bd-81b6-dedfa3397b77",
   "metadata": {},
   "source": [
    "# HINDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "641df33b-4a3a-4875-b382-ad7d76432e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.blank(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2ceed0d-aa08-46c4-aed2-bc78ea4d30b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp2(''' \n",
    "जनगणमन-अधिनायक जय हे भारतभाग्यविधाता!\n",
    "पंजाब सिन्ध गुजरात मराठा द्राविड़ उत्कल बंग\n",
    "विंध्य हिमाचल यमुना गंगा उच्छल जलधि तरंग\n",
    "''' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8748dcb1-7977-427c-bead-31f2bf50a38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "जनगणमन\n",
      "-\n",
      "अधिनायक\n",
      "जय\n",
      "हे\n",
      "भारतभाग्यविधाता\n",
      "!\n",
      "\n",
      "\n",
      "पंजाब\n",
      "सिन्ध\n",
      "गुजरात\n",
      "मराठा\n",
      "द्राविड़\n",
      "उत्कल\n",
      "बंग\n",
      "\n",
      "\n",
      "विंध्य\n",
      "हिमाचल\n",
      "यमुना\n",
      "गंगा\n",
      "उच्छल\n",
      "जलधि\n",
      "तरंग\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bc3440-f548-471f-a4e5-5295fe8ef9b0",
   "metadata": {},
   "source": [
    "# Customizing rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0607a4ad-3dcd-496e-9d90-9e794a9681d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cde9e31-707c-4759-b142-af1f999b5cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', 'double', 'cheeze', 'extra', 'large', 'pizza']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.tokenizer.add_special_case(\"gimme\",[\n",
    "    {ORTH:\"gim\"},\n",
    "    {ORTH:\"me\"}\n",
    "])\n",
    "\n",
    "doc = nlp(\"gimme double cheeze extra large pizza\")\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af42318b-9888-4d56-a7c3-6d94a9bf17a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
